# Руководство по интеграции с Ollama

## Обзор

Данный проект теперь поддерживает интеграцию с Ollama - локальной системой запуска больших языковых моделей. Это позволяет запускать чат-бота без необходимости подключения к внешним API.

## Основные компоненты

### 1. OllamaAPI (`ollama_integration.py`)
Класс для взаимодействия с Ollama API:
- Подключение к локальному серверу Ollama
- Отправка сообщений и генерация текста
- Управление моделями и историей разговоров

### 2. OllamaRAGSystem (`ollama_rag_system.py`)
RAG (Retrieval-Augmented Generation) система на базе Ollama:
- Создание эмбеддингов для документов
- Поиск похожих чанков
- Генерация ответов на основе контекста

### 3. OllamaRAGChatBot (`ollama_integration.py`)
Чат-бот с поддержкой RAG на базе Ollama.

## Установка и настройка

### 1. Установка Ollama

```bash
# Установка Ollama (Linux/macOS)
curl -fsSL https://ollama.ai/install.sh | sh

# Запуск Ollama
ollama serve

# Установка моделей
ollama pull llama3.2:3b
ollama pull nomic-embed-text
```

### 2. Настройка переменных окружения

Скопируйте файл `.env.ollama` в `.env` и настройте параметры:

```bash
# Ollama подключение
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Параметры генерации
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=1024

# RAG система
KNOWLEDGE_BASE_PATH=knowledge_base
MAX_SIMILAR_DOCS=5
SIMILARITY_THRESHOLD=0.7
```

### 3. Установка зависимостей

```bash
# Установка Python зависимостей
pip install -r requirements.txt
```

## Использование

### Базовый запуск

```bash
# Запуск сервера с Ollama интеграцией
python widget_server.py
```

### API Endpoints

#### Проверка состояния Ollama
```http
GET /health
```

#### Получение статуса Ollama
```http
GET /ollama/status
```

#### Получение списка доступных моделей
```http
GET /ollama/models
```

#### Отправка сообщения чат-боту
```http
POST /chat
Content-Type: application/json

{
    "message": "Ваш вопрос",
    "session_id": "уникальный_идентификатор_сессии"
}
```

## Тестирование

### Запуск тестов

```bash
# Запуск всех тестов для Ollama
python test_ollama.py
```

### Проверка компонентов

Тесты покрывают:
- Подключение к Ollama API
- Генерацию текста
- Работу RAG системы
- Интеграцию компонентов

## Особенности реализации

### 1. Многовариантность
Система автоматически переключается между Ollama и существующей RAG системой:
- Если Ollama доступен - используется OllamaRAGSystem
- Если Ollama недоступен - используется стандартная RAG система
- Если обе системы недоступны - возвращается заглушка

### 2. Кэширование эмбеддингов
RAG система сохраняет созданные эмбеддинги в файле `vector_store.json` для ускорения последующих запросов.

### 3. История разговоров
OllamaAPI поддерживает контекст разговора, что делает ответы более релевантными.

## Производительность

### Рекомендации
- Используйте модель `llama3.2:1b` для быстрого ответа
- Для более качественных ответов используйте `llama3.2:3b`
- Убедитесь, что у вас достаточно RAM для выбранной модели

### Мониторинг
Система логирует:
- Время ответа Ollama
- Размер контекста
- Ошибки подключения

## Устранение неполадок

### Ollama не запускается
```bash
# Проверка статуса
systemctl status ollama

# Запуск службы
sudo systemctl start ollama

# Проверка логов
journalctl -u ollama -f
```

### Модель не найдена
```bash
# Просмотр доступных моделей
ollama list

# Установка нужной модели
ollama pull llama3.2:3b
```

### Проблемы с подключением
1. Проверьте, что Ollama запущен: `ollama serve`
2. Проверьте порт 11434: `netstat -tulpn | grep 11434`
3. Убедитесь, что модель установлена
4. Проверьте настройки в `.env` файле

## Безопасность

- Ollama работает локально, что исключает утечку данных наружу
- Убедитесь, что порт 11434 закрыт для внешнего доступа
- Используйте firewall для ограничения доступа

## Дальнейшее развитие

Планируется добавление:
- Поддержки других моделей эмбеддингов
- Оптимизация работы с большими базами знаний
- Поддержка многопользовательского режима
- Интеграция с внешними источниками данных