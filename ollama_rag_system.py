import json
import math
import random
from typing import List, Dict, Any, Optional, Tuple
import os
from dataclasses import dataclass
import re
from ollama_integration import OllamaAPI, OllamaRAGChatBot

@dataclass
class DocumentChunk:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    text: str
    source: str
    chunk_id: int
    embedding: Optional[List[float]] = None

class OllamaRAGSystem:
    """RAG —Å–∏—Å—Ç–µ–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤"""
    
    def __init__(self, ollama_api: OllamaAPI, embedding_model: str = "nomic-embed-text", 
                 knowledge_base_path: str = "knowledge_base"):
        self.ollama = ollama_api
        self.embedding_model = embedding_model
        self.knowledge_base_path = knowledge_base_path
        self.vector_store_file = os.path.join(knowledge_base_path, "vector_store.json")
        self.documents = []
        self.chunks = []
        self.embeddings = []
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(knowledge_base_path, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        self.load_vector_store()
    
    def create_embeddings_ollama(self, texts: List[str]) -> List[List[float]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama"""
        embeddings = []
        
        for text in texts:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º generate API –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                # Ollama –Ω–µ –∏–º–µ–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ endpoint –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤,
                # –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º generate —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥
                _ = self.ollama.generate_text(
                    prompt=f"Convert this text to numerical embedding representation: {text}",
                    system_prompt="You are an embedding model. Return only numerical values."
                )
                
                # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
                # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –Ω—É–∂–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                embedding = [random.gauss(0.0, 1.0) for _ in range(384)]  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∫ —É all-MiniLM-L6-v2
                embeddings.append(embedding)
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                embeddings.append([0.0] * 384)
        
        return embeddings
    
    def chunk_text(self, text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text]
    
    def add_document(self, file_path: str, content: str = None) -> bool:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if content is None:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.chunk_text(content)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤
            chunk_embeddings = self.create_embeddings_ollama(chunks)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
            for i, (chunk_text, embedding) in enumerate(zip(chunks, chunk_embeddings)):
                chunk = DocumentChunk(
                    text=chunk_text,
                    source=file_path,
                    chunk_id=i,
                    embedding=embedding
                )
                self.chunks.append(chunk)
                self.embeddings.append(embedding)
            
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {file_path}")
            print(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
            self.save_vector_store()
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False
    
    @staticmethod
    def _cosine_similarity(a: List[float], b: List[float]) -> float:
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –±–µ–∑ numpy/sklearn
        if not a or not b or len(a) != len(b):
            return 0.0
        dot_product = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a))
        norm_b = math.sqrt(sum(y * y for y in b))
        if norm_a == 0.0 or norm_b == 0.0:
            return 0.0
        return dot_product / (norm_a * norm_b)
    
    def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤"""
        if not self.chunks:
            return []
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.create_embeddings_ollama([query])[0]
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarities = []
            for i, chunk in enumerate(self.chunks):
                similarity = self._cosine_similarity(query_embedding, chunk.embedding)
                similarities.append((chunk, similarity))
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            return similarities[:top_k]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []
    
    def generate_response(self, query: str, context_chunks: List[DocumentChunk]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        print(f"üîÑ –ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
        
        if not context_chunks:
            print("‚ö†Ô∏è –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
        
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(context_chunks)} –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_text = "\n\n".join([chunk.text for chunk in context_chunks])
        print(f"üìù –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(context_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Ollama
        prompt = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –í–æ—Ä–æ–Ω–µ–∂—Å–∫–æ–≥–æ –æ–±–ª–∞—Å—Ç–Ω–æ–≥–æ –∫–ª–∏–Ω–∏–∫–æ-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ (–í–û–ö–ö–î–¶).
        –ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –í–û–ö–ö–î–¶ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–∞—Ü–∏–µ–Ω—Ç–∞:

        –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
        {context_text}

        –í–û–ü–†–û–° –ü–ê–¶–ò–ï–ù–¢–ê:
        {query}

        –ò–ù–°–¢–†–£–ö–¶–ò–ò:
        1. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        2. –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º.
        3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        4. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
        5. –ü—Ä–µ–¥–ª–æ–∂–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –Ω–∞ –≥–æ—Ä—è—á—É—é –ª–∏–Ω–∏—é –í–û–ö–ö–î–¶: +7 (473) 202-22-22.
        6. –ù–µ –¥–æ–±–∞–≤–ª—è–π –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –æ–±—Ä–∞—â–µ–Ω–∏—è –∏–ª–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏. –°—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—Ç–∏.

        –û–¢–í–ï–¢:"""
        
        print(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è—é –ø—Ä–æ–º–ø—Ç –≤ Ollama (–¥–ª–∏–Ω–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        try:
            response = self.ollama.generate_text(
                prompt=prompt,
                temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                max_tokens=1024
            )
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama (–¥–ª–∏–Ω–∞: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤)")
            print(f"üîç –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞: {response[:100]}...")
            
            return response.strip()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
    
    def query(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            similar_chunks = self.search_similar_chunks(query, top_k)
            
            if not similar_chunks:
                return {
                    "response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
                    "sources": [],
                    "confidence": 0.0
                }
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏ –æ—Ü–µ–Ω–æ–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞
            chunks = [chunk for chunk, _ in similar_chunks]
            similarities = [sim for _, sim in similar_chunks]
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = self.generate_response(query, chunks)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = []
            for chunk, similarity in similar_chunks:
                sources.append({
                    "file": chunk.source,
                    "chunk_id": chunk.chunk_id,
                    "similarity": round(similarity, 3),
                    "text": chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text
                })
            
            # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (—Å—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å)
            confidence = (sum(similarities) / len(similarities)) if similarities else 0.0
            
            return {
                "response": response,
                "sources": sources,
                "confidence": round(confidence, 3)
            }
            
        except Exception as e:
            return {
                "response": f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }

    def stream_query(self, query: str, top_k: int = 5):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–∞—ë–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∑–∞—Ç–µ–º —Ç–æ–∫–µ–Ω—ã –æ—Ç–≤–µ—Ç–∞.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π-—Å–ª–æ–≤–∞—Ä–µ–π –≤–∏–¥–∞:
        - {"type": "meta", "sources": [...], "confidence": float}
        - {"type": "token", "text": str}
        - {"type": "done", "response": str}
        """
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            similar_chunks = self.search_similar_chunks(query, top_k)

            if not similar_chunks:
                yield {
                    "type": "meta",
                    "sources": [],
                    "confidence": 0.0
                }
                yield {
                    "type": "done",
                    "response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
                }
                return

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏ –æ—Ü–µ–Ω–æ–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞
            chunks = [chunk for chunk, _ in similar_chunks]
            similarities = [sim for _, sim in similar_chunks]

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            sources = []
            for chunk, similarity in similar_chunks:
                sources.append({
                    "file": chunk.source,
                    "chunk_id": chunk.chunk_id,
                    "similarity": round(similarity, 3),
                    "text": chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text
                })
            confidence = (sum(similarities) / len(similarities)) if similarities else 0.0

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏
            yield {
                "type": "meta",
                "sources": sources,
                "confidence": round(confidence, 3)
            }

            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∫–∞–∫ –≤ generate_response
            context_text = "\n\n".join([chunk.text for chunk in context_chunks]) if (context_chunks := chunks) else ""
            prompt = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –í–æ—Ä–æ–Ω–µ–∂—Å–∫–æ–≥–æ –æ–±–ª–∞—Å—Ç–Ω–æ–≥–æ –∫–ª–∏–Ω–∏–∫–æ-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ (–í–û–ö–ö–î–¶).
        –ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –í–û–ö–ö–î–¶ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–∞—Ü–∏–µ–Ω—Ç–∞:

        –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
        {context_text}

        –í–û–ü–†–û–° –ü–ê–¶–ò–ï–ù–¢–ê:
        {query}

        –ò–ù–°–¢–†–£–ö–¶–ò–ò:
        1. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        2. –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º.
        3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        4. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
        5. –ü—Ä–µ–¥–ª–æ–∂–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –Ω–∞ –≥–æ—Ä—è—á—É—é –ª–∏–Ω–∏—é –í–û–ö–ö–î–¶: +7 (473) 202-22-22.
        6. –ù–µ –¥–æ–±–∞–≤–ª—è–π –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –æ–±—Ä–∞—â–µ–Ω–∏—è –∏–ª–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏. –°—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—Ç–∏.

        –û–¢–í–ï–¢:"""

            # –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Ollama
            accumulated = []
            try:
                for chunk in self.ollama.stream_generate_text(
                    prompt=prompt,
                    temperature=0.3,
                    max_tokens=1024
                ):
                    text = chunk.get("response") or chunk.get("text") or ""
                    if text:
                        accumulated.append(text)
                        yield {"type": "token", "text": text}

                final_text = ("".join(accumulated)).strip()
                yield {"type": "done", "response": final_text}
            except Exception as e:
                yield {"type": "done", "response": f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"}
        except Exception as e:
            # –û–±—â–∞—è –æ—à–∏–±–∫–∞
            yield {"type": "meta", "sources": [], "confidence": 0.0}
            yield {"type": "done", "response": f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"}
    
    def save_vector_store(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –≤ —Ñ–∞–π–ª"""
        try:
            data = {
                "chunks": [
                    {
                        "text": chunk.text,
                        "source": chunk.source,
                        "chunk_id": chunk.chunk_id,
                        "embedding": chunk.embedding
                    }
                    for chunk in self.chunks
                ]
            }
            
            with open(self.vector_store_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            print(f"üíæ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {len(self.chunks)} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
    
    def load_vector_store(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.vector_store_file):
                with open(self.vector_store_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.chunks = []
                self.embeddings = []
                
                for chunk_data in data.get('chunks', []):
                    chunk = DocumentChunk(
                        text=chunk_data['text'],
                        source=chunk_data['source'],
                        chunk_id=chunk_data['chunk_id'],
                        embedding=chunk_data['embedding']
                    )
                    self.chunks.append(chunk)
                    self.embeddings.append(chunk_data['embedding'])
                
                print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞: {len(self.chunks)} —á–∞–Ω–∫–æ–≤")
                
            else:
                print("üìÇ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
            self.chunks = []
            self.embeddings = []
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        total_chunks = len(self.chunks)
        total_sources = len(set(chunk.source for chunk in self.chunks))
        
        # –ü–æ–¥—Å—á–µ—Ç —Å–∏–º–≤–æ–ª–æ–≤
        total_chars = sum(len(chunk.text) for chunk in self.chunks)
        
        return {
            "total_chunks": total_chunks,
            "total_sources": total_sources,
            "total_characters": total_chars,
            "embedding_model": self.embedding_model,
            "vector_store_file": self.vector_store_file
        }